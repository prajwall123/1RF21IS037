import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor

# Task 1: Preprocessing the Dataset
# Assuming you have a dataset named "walmart_sales.csv"
data = pd.read_csv("walmart_sales.csv")

# Perform necessary preprocessing steps (handle missing values, encode categorical variables, etc.)

# Task 2: Visualize the data using specific graphs
# Example: Visualizing the distribution of the target variable
sns.histplot(data['target_variable'], bins=20)
plt.title('Distribution of Target Variable')
plt.show()

# Task 3: Hyperparameter Tuning
# Example: Using GridSearchCV for hyperparameter tuning
X = data.drop(columns=['target_variable'])
y = data['target_variable']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

param_grid = {'max_depth': [3, 5, 7, 10], 'min_samples_split': [2, 5, 10]}
grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_

# Task 4: Root Mean Square (RMS) Value
# Assuming regression, as the term "Root Mean Square" is more common in regression tasks
# Example: Using DecisionTreeRegressor
regressor = DecisionTreeRegressor(max_depth=best_params['max_depth'], min_samples_split=best_params['min_samples_split'])
regressor.fit(X_train, y_train)
y_pred = regressor.predict(X_test)
rms_value = mean_squared_error(y_test, y_pred, squared=False)

# Task 5: Construct a Decision Tree
# Already done in the regression example above

# Print or use the results as needed
print(f'Best Hyperparameters: {best_params}')
print(f'Root Mean Square Value: {rms_value}')